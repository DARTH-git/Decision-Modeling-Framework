@article{Krijkamp2018,
author = {Krijkamp, EM and Alarid-Escudero, F and Enns, EA and Jalal, H and Hunink, MGM and Pechlivanoglou, P},
doi = {10.1177/0272989X18754513},
isbn = {2059348242},
issn = {1552-681X},
journal = {Medical decision making},
keywords = {Markov model,R project,decision-analytic modeling,microsimulation,open source software},
month = {apr},
number = {3},
pages = {400--422},
pmid = {29587047},
title = {{Microsimulation Modeling for Health Decision Sciences Using R: A Tutorial.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29587047},
volume = {38},
year = {2018}
}

@article{Enns2015,
abstract = {BACKGROUND To identify best-fitting input sets using model calibration, individual calibration target fits are often combined into a single goodness-of-fit (GOF) measure using a set of weights. Decisions in the calibration process, such as which weights to use, influence which sets of model inputs are identified as best-fitting, potentially leading to different health economic conclusions. We present an alternative approach to identifying best-fitting input sets based on the concept of Pareto-optimality. A set of model inputs is on the Pareto frontier if no other input set simultaneously fits all calibration targets as well or better. METHODS We demonstrate the Pareto frontier approach in the calibration of 2 models: a simple, illustrative Markov model and a previously published cost-effectiveness model of transcatheter aortic valve replacement (TAVR). For each model, we compare the input sets on the Pareto frontier to an equal number of best-fitting input sets according to 2 possible weighted-sum GOF scoring systems, and we compare the health economic conclusions arising from these different definitions of best-fitting. RESULTS For the simple model, outcomes evaluated over the best-fitting input sets according to the 2 weighted-sum GOF schemes were virtually nonoverlapping on the cost-effectiveness plane and resulted in very different incremental cost-effectiveness ratios ({\$}79,300 [95{\%} CI 72,500-87,600] v. {\$}139,700 [95{\%} CI 79,900-182,800] per quality-adjusted life-year [QALY] gained). Input sets on the Pareto frontier spanned both regions ({\$}79,000 [95{\%} CI 64,900-156,200] per QALY gained). The TAVR model yielded similar results. CONCLUSIONS Choices in generating a summary GOF score may result in different health economic conclusions. The Pareto frontier approach eliminates the need to make these choices by using an intuitive and transparent notion of optimality as the basis for identifying best-fitting input sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Enns, EA and Cipriano, LE and Simons, CT and Kong, CY},
doi = {10.1177/0272989X14528382},
eprint = {arXiv:1011.1669v3},
file = {:Users/elinekrijkamp/Library/Application Support/Mendeley Desktop/Downloaded/Enns et al. - 2015 - Identifying best-fitting inputs in health-economic model calibration a Pareto frontier approach.pdf:pdf},
isbn = {9788578110796},
issn = {0272-989X},
journal = {Medical Decision Making},
keywords = {Pareto frontier,calibration,calibration uncertainty,health-economic model,model uncertainty,parameter estimation,parameter uncertainty},
month = {feb},
number = {2},
pages = {170--182},
pmid = {25246403},
title = {{Identifying Best-Fitting Inputs in Health-Economic Model Calibration: A Pareto Frontier Approach}},
url = {http://mdm.sagepub.com/cgi/doi/10.1177/0272989X14528382},
volume = {35},
year = {2015}
}


@article{Menzies2017,
author = {Menzies, Nicolas A and Pandya, Ankur and Kim, Jane J},
doi = {10.1007/s40273-017-0494-4.Bayesian},
file = {:Users/elinekrijkamp/Dropbox (Persoonlijk)/03{\_}Onderzoek/Mendeley/Menzies, Pandya, Kim - 2017 - HHS Public Access.pdf:pdf},
journal = {Pharmacoeconomics},
number = {6},
pages = {613--624},
title = {{HHS Public Access}},
volume = {35},
year = {2017}
}

@article{Nelder1965,
    author = {Nelder, J. A. and Mead, R.},
    title = "{A Simplex Method for Function Minimization}",
    journal = {The Computer Journal},
    volume = {7},
    number = {4},
    pages = {308-313},
    year = {1965},
    month = {01},
    abstract = "{A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/7.4.308},
    url = {https://dx.doi.org/10.1093/comjnl/7.4.308},
    eprint = {http://oup.prod.sis.lan/comjnl/article-pdf/7/4/308/1013182/7-4-308.pdf},
}

@article{Alarid-Escudero2018b,
author = {Fernando Alarid-Escudero and Richard F. MacLehose and Yadira Peralta and Karen M. Kuntz and Eva A. Enns},
title ={Nonidentifiability in Model Calibration and Implications for Medical Decision Making},
journal = {Medical Decision Making},
volume = {38},
number = {7},
pages = {810-821},
year = {2018},
doi = {10.1177/0272989X18792283},
    note ={PMID: 30248276},

URL = { 
        https://doi.org/10.1177/0272989X18792283
    
},
eprint = { 
        https://doi.org/10.1177/0272989X18792283
    
}
,
    abstract = { Background. Calibration is the process of estimating parameters of a mathematical model by matching model outputs to calibration targets. In the presence of nonidentifiability, multiple parameter sets solve the calibration problem, which may have important implications for decision making. We evaluate the implications of nonidentifiability on the optimal strategy and provide methods to check for nonidentifiability. Methods. We illustrate nonidentifiability by calibrating a 3-state Markov model of cancer relative survival (RS). We performed 2 different calibration exercises: 1) only including RS as a calibration target and 2) adding the ratio between the 2 nondeath states over time as an additional target. We used the Nelder-Mead (NM) algorithm to identify parameter sets that best matched the calibration targets. We used collinearity and likelihood profile analyses to check for nonidentifiability. We then estimated the benefit of a hypothetical treatment in terms of life expectancy gains using different, but equally good-fitting, parameter sets. We also applied collinearity analysis to a realistic model of the natural history of colorectal cancer. Results. When only RS is used as the calibration target, 2 different parameter sets yield similar maximum likelihood values. The high collinearity index and the bimodal likelihood profile on both parameters demonstrated the presence of nonidentifiability. These different, equally good-fitting parameter sets produce different estimates of the treatment effectiveness (0.67 v. 0.31 years), which could influence the optimal decision. By incorporating the additional target, the model becomes identifiable with a collinearity index of 3.5 and a unimodal likelihood profile. Conclusions. In the presence of nonidentifiability, equally likely parameter estimates might yield different conclusions. Checking for the existence of nonidentifiability and its implications should be incorporated into standard model calibration procedures. }
}


@Manual{IMIS,
    title = {IMIS:  Incremental Mixture Importance Sampling},
    author = {Adrian Raftery and {Le Bao}},
    year = {2012},
    note = {R package version 0.1},
    url = {https://CRAN.R-project.org/package=IMIS},
  }

@article{Raftery2010,
author = {{Raftery, A., Bao}, L.},
file = {:Users/elinekrijkamp/Downloads/nihms-187290.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {1162--1173},
title = {{Estimating and Projecting Trends in HIV/AIDS Generalized Epidemics Using Incremental Mixture Importance Sampling}},
volume = {66},
year = {2010}
}

@article{Alarid-Escudero2019b,
author = {Alarid-Escudero, F and Krijkamp, EM and Pechlivanoglou, P  and Jalal, H and Enns, EA},
journal = {Submitted to PharmacoEconomics},
keywords = {XX},
number = {X},
pages = {XX},
title = {{A need for change! A coding framework for improving transparency in decision modeling}},
url = {XX},
volume = {XX},
year = {2019}
}

@article{Eddy2012,
author = {Eddy, David M. and Hollingworth, William and Caro, J. Jaime and Tsevat, Joel and McDonald, Kathryn M. and Wong, John B.},
doi = {10.1177/0272989X12454579},
isbn = {1524-4733},
issn = {0272989X},
journal = {Medical Decision Making},
keywords = {decision sciences,good practices,modeling,simulation,transparency,validation},
number = {5},
pages = {733--743},
pmid = {22999134},
title = {{Model transparency and validation: A report of the ISPOR-SMDM modeling good research practices task force-7}},
volume = {32},
year = {2012}
}

@article{Goldhaber-Fiebert2010,
author = {Goldhaber-Fiebert, Jeremy D. and Stout, Natasha K. and Goldie, Sue J.},
doi = {10.1111/j.1524-4733.2010.00698.x},
isbn = {1098-3015; 1524-4733},
issn = {15244733},
journal = {Value in Health},
keywords = {HPV,cancer,cervical cancer,cost-effectiveness,literature review,methods,microsimulation model,simulation model,validation,validity},
mendeley-groups = {CE15-2018,CE02 Prognosis},
mendeley-tags = {HPV,cervical cancer,literature review,microsimulation model,validity},
number = {5},
pages = {667--674},
pmid = {20230547},
title = {{Empirically evaluating decision-analytic models}},
volume = {13},
year = {2010}
}

@article{Iskandar2018,
abstract = {Following its introduction over three decades ago, the cohort model has been used extensively to model population trajectories over time in decision-analytic modeling studies. However, the stochastic process underlying cohort models has not been properly described. In this study, we explicate the stochastic process underlying a cohort model, by carefully formulating the dynamics of populations across health states and assigning probability rules on these dynamics. From this formulation, we explicate a mathematical representation of the system, which is given by the master equation. We solve the master equation by using the probability generation function method to obtain the explicit form of the probability of observing a particular realization of the system at an arbitrary time. The resulting generating function is used to derive the analytical expressions for calculating the mean and the variance of the process. Secondly, we represent the cohort model by a difference equation for the number of individuals across all states. From the difference equation, a continuous-time cohort model is recovered and takes the form of an ordinary differential equation. To show the equivalence between the derived stochastic process and the cohort model, we conduct a numerical exercise. We demonstrate that the population trajectories generated from the formulas match those from the cohort model simulation. In summary, the commonly-used cohort model represent the average of a continuous-time stochastic process on a multidimensional integer lattice governed by a master equation. Knowledge of the stochastic process underlying a cohort model provides a theoretical foundation for the modeling method.},
author = {Iskandar, Rowan},
doi = {10.1371/journal.pone.0205543},
issn = {1932-6203},
journal = {PloS one},
month = {dec},
number = {12},
pages = {e0205543--e0205543},
publisher = {Public Library of Science},
title = {{A theoretical foundation for state-transition cohort models in health decision analysis}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/30533043 https://www.ncbi.nlm.nih.gov/pmc/PMC6289421/},
volume = {13},
year = {2018}
}


@Manual{modeest,
    title = {modeest: Mode Estimation},
    author = {Paul Poncet},
    year = {2018},
    note = {R package version 2.3.2},
    url = {https://CRAN.R-project.org/package=modeest},
  }

